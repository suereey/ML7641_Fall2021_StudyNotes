# Information Theory

## Intro
![01](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/01.PNG)

## Example: sending message. Information in 1 variable
- Example 1:
    ![02](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/02.PNG)
    - if the answer is random: you need to send
    - if the result is not random (known): no need to send
- Example 2:
    ![03](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/03.PNG)
- Example 3:
    ![04](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/04.PNG)


## Information between variables 
- joint entropy
- conditional entorpy
![05](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/05.PNG)
- mutal information: measure of the reduction of randomess of variable given the knowledge of some other varaibles
![06](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/06.PNG)
- Examples
    ![07](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/07.PNG)
    ![08](https://raw.githubusercontent.com/suereey/ML7641_Fall2021_StudyNotes/main/Screenshot/UL5/08.PNG)

## Kullback-leibler Divergence (ke Divergence)

## Summary
![10]()